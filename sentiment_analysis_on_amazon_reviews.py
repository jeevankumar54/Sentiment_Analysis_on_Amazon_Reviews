# -*- coding: utf-8 -*-
"""Sentiment Analysis on Amazon Reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YYfmIRUYrXpxm92pm4bAZO_cMpRfU2bm
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.stem.porter import PorterStemmer
nltk.download('stopwords')
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from wordcloud import WordCloud
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
import pickle
import re

data = pd.read_csv(r"/content/amazon_alexa.tsv", delimiter = '\t', quoting = 3)

print(f"Dataset shape : {data.shape}")

data.head()

print(f"Feature names: {data.columns.values}")

data.isnull().sum()

data[data['verified_reviews'].isna() == True]

data.dropna(inplace = True)

data.shape

data['length'] = data['verified_reviews'].apply(len)

data.head()

data.dtypes

"""Analysing 'rating' column"""

data['rating'].value_counts()

plt.figure(figsize = (5, 3))
data['rating'].value_counts().plot.bar(color = 'red')
plt.title("Rating distribution count")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()

print(f"percentage distribution: \n{data['rating'].value_counts(normalize = True) * 100}")

colors = ('red', 'green', 'blue', 'orange', 'yellow')
wp = {'linewidth': 1, 'edgecolor': 'black'}
tags = data['rating'].value_counts()/data.shape[0]
explode = (0.1, 0.1, 0.1, 0.1, 0.1)
plt.figure(figsize = (5, 5))
plt.pie(tags, autopct = '%1.1f%%', explode = explode, labels = ['5', '4', '3', '2', '1'], colors = colors, wedgeprops = wp)
plt.title("Percentage distribution of rating")
plt.show()

"""Analyzing 'feedback' column"""

data['feedback'].value_counts()

plt.figure(figsize = (5, 3))
data['feedback'].value_counts().plot.bar(color = 'blue')
plt.title("Feedback distribution count")
plt.xlabel("Feedback")
plt.ylabel("Count")
plt.show()

data[data['feedback'] == 0]['rating'].value_counts()

data[data['feedback'] == 1]['rating'].value_counts()

"""Analysing 'variation' column"""

data['variation'].value_counts()

plt.figure(figsize = (5, 3))
data['variation'].value_counts().plot.bar(color = 'green')
plt.title("Variation distribution count")
plt.xlabel("Variation")
plt.ylabel("Count")
plt.show()

data.groupby('variation')['rating'].mean()

data.groupby('variation')['rating'].mean().sort_values().plot.bar(color = 'orange', figsize=(5, 3))
plt.title("Variation rating mean")
plt.xlabel("Variation")
plt.ylabel("Rating mean")
plt.show()

"""Analyzing 'verified_reviews'column"""

data['length'].describe()

cv = CountVectorizer(stop_words = 'english')
words = cv.fit_transform(data['verified_reviews'])

# Combine all reviews
reviews = " ".join([review for review in data['verified_reviews']])

# Initialize wordcloud object
wc = WordCloud(width = 800, height = 500, background_color = 'white', max_words=50, stopwords = STOPWORDS)

# Generate wordcloud
plt.figure(figsize=(5,5))
plt.imshow(wc.generate(reviews))
plt.title('Wordcloud for all reviews', fontsize=10)
plt.axis('off')
plt.show()

"""# Preprocessing and Modelling

To build the corpus from the 'verified_reviews' we perform the following

1. Replace any non alphabet characters with a space
2. Convert to lower case and split into words
3. Iterate over the individual words and if it is not a stopword then add the stemmed form of the word to the corpus
"""

corpus = []
stemmer = PorterStemmer()
for i in range(0, data.shape[0]):
  review = re.sub('[^a-zA-Z]', ' ', data.iloc[i]['verified_reviews'])
  review = review.lower().split()
  review = [stemmer.stem(word) for word in review if not word in STOPWORDS]
  review = ' '.join(review)
  corpus.append(review)

"""Using Count Vectorizer to create bag of words"""

cv = CountVectorizer(max_features = 2500)
X = cv.fit_transform(corpus).toarray()
y = data['feedback'].values

# Saving the Count Vectorizer
pickle.dump(cv, open('countVectorizer.pkl', 'wb'))

print(X.shape)
y.shape

"""Splitting into training and test data"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 15)

scaler = MinMaxScaler()
X_train_scl = scaler.fit_transform(X_train)
X_test_scl = scaler.transform(X_test)

# Saving the scaler model
pickle.dump(scaler, open('scaler.pkl', 'wb'))

"""**Random Forest**"""

# Fitting scaled X_train and y_train on Random Forest Classifier
model_rf = RandomForestClassifier()
model_rf.fit(X_train_scl, y_train)

# Accuracy on training and test data

print(f"Accuracy on training data: {model_rf.score(X_train_scl, y_train)}")
print(f"Accuracy on test data: {model_rf.score(X_test_scl, y_test)}")

# Predicting on the test set
y_pred = model_rf.predict(X_test_scl)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_rf.classes_).plot()
plt.show()

"""K fold cross-validation"""

accuracies = cross_val_score(estimator = model_rf, X = X_train_scl, y = y_train, cv = 10)
print(f"Accuracy: {accuracies.mean()*100} %")
print(f"Standard Deviation: {accuracies.std()*100} %")

"""Applying grid search to get the optimal parameters on random forest"
"""

params = {
    'bootstrap': [True],
    'max_depth': [80, 100],
    'min_samples_split': [8, 12],
    'n_estimators': [100, 300]
}

cv_object = StratifiedKFold(n_splits=2)

grid_search = GridSearchCV(estimator = model_rf, param_grid = params, cv = cv_object, verbose = 0, return_train_score= True)
grid_search.fit(X_train_scl, y_train.ravel())

# Getting the best parameter from the grid search

print(grid_search.best_params_)

print("Cross validation mean accuracy on train set: {}".format(grid_search.cv_results_['mean_train_score'].mean()*100))
print("Cross validation mean accuracy on test set: {}".format(grid_search.cv_results_['mean_test_score'].mean()*100))
print("Accuracy score for test set:", accuracy_score(y_test, y_pred))

"""**XGBoost**"""

model_xgb = XGBClassifier()
model_xgb.fit(X_train_scl, y_train)

# Accuracy of model on training and test data

print(f"Accuracy on training data: {model_xgb.score(X_train_scl, y_train)}")
print(f"Accuracy on test data: {model_xgb.score(X_test_scl, y_test)}")

y_preds = model_xgb.predict(X_test_scl)

# Confusion Matrix
cm = confusion_matrix(y_test, y_preds)
print(cm)

cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_xgb.classes_).plot()
plt.show()

# Saving the XGBoost classifier
pickle.dump(model_xgb, open('model_xgb.pkl', 'wb'))

"""**Decision Tree Classifier**"""

model_dt = DecisionTreeClassifier()
model_dt.fit(X_train_scl, y_train)

# Accuracy on traing and test

print(f"Accuracy on training data: {model_dt.score(X_train_scl, y_train)}")
print(f"Accuracy on test data: {model_dt.score(X_test_scl, y_test)}")

y_preds = model_dt.predict(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_preds)
print(cm)

cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_dt.classes_).plot()
plt.show()

pickle.dump(model_rf, open('model_rf.pkl', 'wb'))

